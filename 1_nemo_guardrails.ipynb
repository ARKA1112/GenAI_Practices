{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##kernel start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install  nemoguardrails\n",
    "#%pip install python-dotenv\n",
    "#%pip install openai\n",
    "#%conda install numpy=1.24.0\n",
    "\n",
    "\n",
    "#%pip install -qU  nemoguardrails==0.4.0 openai==0.27.8\n",
    "\n",
    "#%pip install git+https://github.com/NVIDIA/NeMo-Guardrails.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by import the basic packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two ways to go about from here\n",
    "  * create two files called `config.yml` and `topics.co` and put some configurations inside it\n",
    "  * create yaml_content and colang content on the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with load dotenv the environments are already loaded\n",
    "# here we create the configs on the run\n",
    "\n",
    "\n",
    "yaml_content = \"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: text-davinci-003\n",
    "\"\"\"\n",
    "\n",
    "colang_content = \"\"\"\n",
    "# define niceties\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "    \"what's up?\"\n",
    "    \"namaskaram\"\n",
    "\n",
    "define flow greeting\n",
    "    user express greeting\n",
    "    bot express greeting\n",
    "    bot ask how are you\n",
    "\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a shopping assistant, I don't like to talk of politics\"\n",
    "\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with the nemoguardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")\n",
    "\n",
    "# create the rails\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rails = LLMRails(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our chatbot is ready we can ask it questions and check if the guardrails are properly implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error Incorrect API key provided: sk-5YG98***************************************x0ve. You can find your API key at https://platform.openai.com/account/api-keys. while execution generate_user_intent\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/nemoguardrails/actions/action_dispatcher.py\", line 177, in execute_action\n",
      "    result = await result\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/nemoguardrails/actions/llm/generation.py\", line 311, in generate_user_intent\n",
      "    result = await llm_call(llm, prompt)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/nemoguardrails/actions/llm/utils.py\", line 53, in llm_call\n",
      "    result = await llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 535, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 893, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 757, in _agenerate_helper\n",
      "    raise e\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 741, in _agenerate_helper\n",
      "    await self._agenerate(\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_community/llms/openai.py\", line 523, in _agenerate\n",
      "    response = await acompletion_with_retry(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_community/llms/openai.py\", line 142, in acompletion_with_retry\n",
      "    return await _completion_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_community/llms/openai.py\", line 140, in _completion_with_retry\n",
      "    return await llm.client.acreate(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_resources/completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 726, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.AuthenticationError: Incorrect API key provided: sk-5YG98***************************************x0ve. You can find your API key at https://platform.openai.com/account/api-keys.\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, an internal error has occurred.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
