{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "colang_content = \"\"\"\n",
    "define user greeting\n",
    "    \"Hey there!\"\n",
    "    \"How are you?\"\n",
    "    \"What's up?\"\n",
    "    \n",
    "define bot name greeting\n",
    "    \"Hey $name!\"\n",
    "\n",
    "\n",
    "define flow\n",
    "    user greeting\n",
    "    if $name\n",
    "        bot name greeting\n",
    "    else\n",
    "        bot greeting\n",
    "        \n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "    \"\"\"\n",
    "\n",
    "yaml_content = \"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: gpt-3.5-turbo-16k\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    colang_content=colang_content,\n",
    "    yaml_content=yaml_content\n",
    ")\n",
    "\n",
    "#create rails\n",
    "\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role':'context', \"content\":\"\"},\n",
    "    {'role':'user',\"content\":\"Hey there!\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': 'Hey there! How can I assist you today?'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rails.generate_async(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"I can definitely help you with that! To find a political party to join, I would recommend considering your own political beliefs and values. You can research different political parties and their platforms to see which aligns most closely with your own views. Additionally, you can attend local political events or meetings to learn more about different parties and connect with like-minded individuals. It's important to be an informed voter and actively participate in the political process. Let me know if there's anything specific you'd like assistance with!\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"context\", \"content\": {\"name\": \"Arka\"}},\n",
    "    {\"role\": \"assistant\",\n",
    "     \"content\": \"Hi! How are you? Is there anything I can help with?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Find me a political party to join\"}\n",
    "]\n",
    "     \n",
    "await rails.generate_async(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the above answer we can clearly assume that we need to provide more context to it in order to deal with political issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RailsConfig.from_content(\n",
    "    colang_content=colang_content,\n",
    "    yaml_content=yaml_content\n",
    ")\n",
    "\n",
    "\n",
    "#create rails\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"I'm sorry, but I can't assist with finding a political party for you to join. It's a personal decision that depends on your beliefs, values, and priorities. I recommend doing some research and exploring different political parties to find one that aligns with your views.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await rails.generate_async(messages=messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-16k in organization org-nXqwFsbc9DcAFxyTEkiHXwz6 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Error Rate limit reached for gpt-3.5-turbo-16k in organization org-nXqwFsbc9DcAFxyTEkiHXwz6 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing. while execution generate_next_step\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/nemoguardrails/actions/action_dispatcher.py\", line 177, in execute_action\n",
      "    result = await result\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/nemoguardrails/actions/llm/generation.py\", line 468, in generate_next_step\n",
      "    result = await llm_call(llm, prompt)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/nemoguardrails/actions/llm/utils.py\", line 53, in llm_call\n",
      "    result = await llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 553, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 513, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 616, in _agenerate_with_cache\n",
      "    return await self._agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_community/chat_models/openai.py\", line 530, in _agenerate\n",
      "    response = await acompletion_with_retry(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_community/chat_models/openai.py\", line 114, in acompletion_with_retry\n",
      "    return await _completion_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/langchain_community/chat_models/openai.py\", line 112, in _completion_with_retry\n",
      "    return await llm.client.acreate(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 217, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 382, in arequest\n",
      "    resp, got_stream = await self._interpret_async_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 726, in _interpret_async_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/susearc/Macos/py3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-16k in organization org-nXqwFsbc9DcAFxyTEkiHXwz6 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': \"I'm sorry, an internal error has occurred.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages += [\n",
    "    res,\n",
    "    {'role':'user','content':\"Im Arka\"}\n",
    "]\n",
    "\n",
    "res = await rails.generate_async(messages=messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
